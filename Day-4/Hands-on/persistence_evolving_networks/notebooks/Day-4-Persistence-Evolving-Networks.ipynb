{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern and Anomaly Mining in Large-scale, Evolving Graphs\n",
    "\n",
    "This tutorial explores pattern mining in evolving graphs (networks), with a focus on anomaly detection. We will explore different statistics that can be computed on patterns, such as frequency and persistence, and their relationships. This notebook is based on content in \"Mining Persistent Activity in Continually Evolving Networks\"\n",
    "https://gemslab.github.io/papers/belth-2020-KDD-PENminer.pdf.\n",
    "\n",
    "Before you start, you'll need to have `numpy`, `scipy`, and `matplotlib` installed. You'll also need to `unzip` all the `.zip` files in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import random\n",
    "from sklearn.neighbors import KDTree\n",
    "from time import time\n",
    "sys.path.append('../src/')\n",
    "'''\n",
    "Below this are imports of custom code for this tutorial.\n",
    "'''\n",
    "from stream import Stream\n",
    "from oPENminer import oPENminer\n",
    "from sPENminer import sPENminer\n",
    "from plotter import Plotter\n",
    "from output_reader import OutputReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a wrapper around Matplotlib with some basic functionality for specific plots that we'll need.\n",
    "See `src/plotter.py` if you are interested. We also initialize a few colors that we'll use throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = Plotter()\n",
    "gray = (0.5019607843137255, 0.5019607843137255, 0.5019607843137255, 0.2) # 0.2 alpha gray\n",
    "black = '#000000'\n",
    "blue = '#377eb8'\n",
    "orange = 'coral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transportation Networks\n",
    "\n",
    "We'll start with transportation networks. These can encode many different types of information, but we will focus on traffic, as this evolves rapidly. Thus an edge (u, v) in a network means that a vehicle is currently traveling from some location u to some other location v. \n",
    "\n",
    "Our first network will be a network of bike share traffic in Columbus, OH. Each node is a bike station from which people can rent bikes.\n",
    "\n",
    "Since once a bike trip is complete the rider leaves the bike, sequences of edges are not linked.\n",
    "Thus, we set the maximum snippet size to, so that each activity snippet corresponds to a single bike trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_name = 'columbus_bike'\n",
    "# a class to feed edge-updates to PENminer\n",
    "stream = Stream(stream_name)\n",
    "# PENminer (the 'o' at the beginning is for offline analysis;\n",
    "# there is also sPENminer if we wanted to analyze a network evolving in real-time)\n",
    "method = oPENminer(stream,\n",
    "                   window_size=1,\n",
    "                   max_size=1,\n",
    "                   view='id',\n",
    "                   alpha=2.0,\n",
    "                   beta=0.5,\n",
    "                   gamma=3.0,\n",
    "                   save_output=True,\n",
    "                   save_occs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run PENminer to compute the frequency and persistence of each snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method.mine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output goes to various files in `output/offline/id/`. If you wish, you can look there to see what exactly it outputs (see the last line of the output for the exact file prefix), but this is unecessary. We built a class that handles reading the output for easy analysis. \n",
    "\n",
    "Run the next cell to load the output you just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_reader = OutputReader(stream_name, \n",
    "                             offline_online='offline', \n",
    "                             window_size=method.window_size, \n",
    "                             max_size=method.max_size, \n",
    "                             view=method.view,\n",
    "                             alpha=method.alpha,\n",
    "                             beta=method.beta, \n",
    "                             gamma=method.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of PENminer is the frequency and persistence of each snippet that appeared in the network over time. Let's start by looking at the relationship between frequency and persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_scores = list()\n",
    "F_scores = list()\n",
    "colors = list()\n",
    "for snippet in output_reader.patterns:\n",
    "    P = output_reader.P(snippet)\n",
    "    F = output_reader.F(snippet)\n",
    "    P_scores.append(P)\n",
    "    F_scores.append(F)\n",
    "    colors.append(gray)\n",
    "plotter.x_vs_y(x=P_scores, \n",
    "               y=F_scores,\n",
    "               title='Columbus Bike PvF plot',\n",
    "               yscale='symlog',\n",
    "               xlabel='Persistence',\n",
    "               ylabel='Frequency',\n",
    "               dpi=250,\n",
    "               size=350,\n",
    "               colors=colors,\n",
    "               transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some positive correlation is visible, as would likely be expected, since frequency is one component of persistence. \n",
    "This cloud of points in and of itself might not be very informative. Let's dig into some of the specific points to learn more.\n",
    "\n",
    "We'll find some anomalies (i.e., outliers) in the PvF plot. To do so, we'll use a simple anomaly detection approach: average KNN. This approach gives a point, `x`, an anomaly score equal to the average Euclidean distance from its k-nearest-neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X is an n x 2 matrix, where each row corresponds to a snippet, the 1st column is its persistence, and the 2nd its frequency\n",
    "X = list()\n",
    "id_to_snippet = dict()\n",
    "for i, snippet in enumerate(output_reader.patterns):\n",
    "    id_to_snippet[i] = snippet\n",
    "    P = output_reader.P(snippet)\n",
    "    F = output_reader.F(snippet, _log=True)\n",
    "    X.append((P, F))\n",
    "X = np.asarray(X)\n",
    "\n",
    "# c.f. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html\n",
    "# A KDTree is a data structure for efficient near-neighbor search.\n",
    "kdtree = KDTree(X)\n",
    "k = 10\n",
    "# The matrix D will be the distance from each point to its k-nearest-neighbors.\n",
    "# The function includes the point itself as a neighbor, so we use k + 1 and will skip the nearest neighbor (trivially the point itself)\n",
    "D, _ = kdtree.query(X, k=k + 1)\n",
    "# we now map each snippet to its anomaly score\n",
    "snippet_to_anom_score = dict()\n",
    "for i, d in enumerate(D):\n",
    "    # the anomaly score of each snippet is the average distance to its k nearest neighbors\n",
    "    snippet_to_anom_score[id_to_snippet[i]] = np.mean(d[1:])\n",
    "\n",
    "# we'll visualize the 10 most anomalous points in coral (orange)\n",
    "top_k = set(it[0] for it in sorted(snippet_to_anom_score.items(), reverse=True, key=lambda it: it[-1])[:10])\n",
    "\n",
    "# generate the points for Matplotlib\n",
    "P_scores = list()\n",
    "F_scores = list()\n",
    "colors = list()\n",
    "for snippet in output_reader.patterns:\n",
    "    P = output_reader.P(snippet)\n",
    "    F = output_reader.F(snippet)\n",
    "    P_scores.append(P)\n",
    "    F_scores.append(F)\n",
    "    if snippet in top_k:\n",
    "        colors.append('coral')\n",
    "    else:\n",
    "        colors.append((0.5019607843137255, 0.5019607843137255, 0.5019607843137255, 0.2)) # 0.2 alpha gray    \n",
    "        \n",
    "# generate the plot \n",
    "plotter.x_vs_y(x=P_scores, \n",
    "               y=F_scores,\n",
    "               title='Columbus Bike PvF plot',\n",
    "               yscale='symlog',\n",
    "               xlabel='Persistence',\n",
    "               ylabel='Frequency',\n",
    "               dpi=250,\n",
    "               size=350,\n",
    "               colors=colors,\n",
    "               transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found some anomalies, let's look at the timeline of some of them to see whatsorts of timelines each region corresponds to. For reference, we'll also show some points in the middle of the PvF plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_scores = list()\n",
    "F_scores = list()\n",
    "X = list()\n",
    "colors = list()\n",
    "for snippet in output_reader.patterns:\n",
    "    P = output_reader.P(snippet)\n",
    "    F = output_reader.F(snippet)\n",
    "    P_scores.append(P)\n",
    "    F_scores.append(F)\n",
    "    X.append([P, output_reader.F(snippet, _log=True)])\n",
    "    if snippet in top_k and P > 10.5: # These will be the super persistent snippets (like the outliers on the right)\n",
    "        colors.append(black)\n",
    "    elif snippet in top_k and P < 1.0 and 500 < F: # these are like the outliers on the left \n",
    "        colors.append(orange)\n",
    "    elif 7.8 < P < 8.1 and 210 < F < 300: # these will be snippets in the middle of the plot for reference\n",
    "        colors.append(blue) # theme blue\n",
    "    else:\n",
    "        colors.append((0.5019607843137255, 0.5019607843137255, 0.5019607843137255, 0.2)) # 0.2 alpha gray\n",
    "X = np.asarray(X)\n",
    "\n",
    "plotter.x_vs_y(x=P_scores, \n",
    "               y=F_scores,\n",
    "               title='Columbus Bike PvF Plot',\n",
    "               yscale='symlog',\n",
    "               xlabel='Persistence',\n",
    "               ylabel='Frequency',\n",
    "               dpi=250,\n",
    "               size=350,\n",
    "               colors=colors,\n",
    "               transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll plot three timelines—one from each colored region—chosen arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The three snippets hand-picked by visualy inspecting the PvF plot.\n",
    "'''\n",
    "output_reader.load_occs({'1_Bicentennial Park_Bicentennial Park_0',\n",
    "                         '1_Front St & Town St_Lucas St & Town St_0',\n",
    "                         '1_Bicentennial Park_Bank St & Frankfort St_0'})\n",
    "t = output_reader.te - output_reader.ts + 1\n",
    "for pattern, occs in output_reader.pattern_to_occs.items():\n",
    "    P = output_reader.P(pattern)\n",
    "    F = output_reader.F(pattern)\n",
    "    if pattern == '1_Bicentennial Park_Bicentennial Park_0':\n",
    "        plotter.timeline(occs,\n",
    "                         ts=1,\n",
    "                         te=t,\n",
    "                         t=t,\n",
    "                         xlabel='',\n",
    "                         interval='Y',\n",
    "                         alpha=1.0,\n",
    "                         timeticksize=1,\n",
    "                         color=black,\n",
    "                         no_labels=True,\n",
    "                         marker='line')\n",
    "    elif pattern == '1_Front St & Town St_Lucas St & Town St_0':\n",
    "        plotter.timeline(occs,\n",
    "                         ts=1,\n",
    "                         te=t,\n",
    "                         t=t,\n",
    "                         xlabel='',\n",
    "                         interval='Y',\n",
    "                         alpha=1.0,\n",
    "                         timeticksize=0.1,\n",
    "                         color=orange,\n",
    "                         no_labels=True,\n",
    "                         marker='line')\n",
    "    elif pattern == '1_Bicentennial Park_Bank St & Frankfort St_0':\n",
    "        plotter.timeline(occs,\n",
    "                         ts=1,\n",
    "                         te=t,\n",
    "                         t=t,\n",
    "                         xlabel='',\n",
    "                         interval='Y',\n",
    "                         alpha=1.0,\n",
    "                         timeticksize=1,\n",
    "                         color=blue,\n",
    "                         no_labels=True,\n",
    "                         marker='line')\n",
    "        \n",
    "# replot the PvF plot just for reference\n",
    "plotter.x_vs_y(x=P_scores, \n",
    "               y=F_scores,\n",
    "               title='Columbus Bike PvF Plot',\n",
    "               yscale='symlog',\n",
    "               xlabel='Persistence',\n",
    "               ylabel='Frequency',\n",
    "               dpi=250,\n",
    "               size=350,\n",
    "               colors=colors,\n",
    "               transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to social networks and do similar analysis. Social networks capture activity among users and/or content. In our case, we'll focus on Stackoverflow, which consists of interactions among users. The interactions are between users, and\n",
    "can be answers to questions, comments on questions, or comments on answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_name = 'stackoverflow'\n",
    "stream = Stream(stream_name)\n",
    "method = oPENminer(stream,\n",
    "                   window_size=3600,\n",
    "                   max_size=3,\n",
    "                   view='id',\n",
    "                   alpha=1.0,\n",
    "                   beta=0.5,\n",
    "                   gamma=2.0,\n",
    "                   save_output=True,\n",
    "                   save_occs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and mine this network. This might take a couple minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method.mine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_reader = OutputReader(stream_name, \n",
    "                             offline_online='offline', \n",
    "                             window_size=method.window_size, \n",
    "                             max_size=method.max_size, \n",
    "                             view=method.view,\n",
    "                             alpha=method.alpha,\n",
    "                             beta=method.beta, \n",
    "                             gamma=method.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is pretty big, so we are going to filter out some of the snippets with both very low persistence and and frequency to allow this to run on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "idx = 0\n",
    "# covered snippets will be the ones we keep\n",
    "covered_snippets = set()\n",
    "for pattern in output_reader.patterns:\n",
    "    P = output_reader.P(pattern)\n",
    "    if P > 1.0 or (P > 0.1 and random.randint(0, 9) == 0) or random.randint(0, 149) == 0:\n",
    "        F = output_reader.F(pattern)\n",
    "        covered_snippets.add(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to demonstrate a different simple approach to detecting anomalies: a density-based approach. We'll generate a set of uniform bins for both frequency and persistence. When these cross each other in the PvF plot, they form a grid, and each point will fall in one cell in the grid. The anomaly score for a point `x` will be the number of other points in the grid (fewer = more anomalous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the maximum F and P values\n",
    "max_F = max(np.log10(np.asarray(list(output_reader.pattern_to_F.values())) + 1))\n",
    "max_P = max(output_reader.pattern_to_P.values())\n",
    "# compute the corresponding bins at intervals of 0.25\n",
    "bins_P = np.arange(0, max_P + 1, 0.25)\n",
    "bins_F = np.arange(0, max_F + 1, 0.25)\n",
    "\n",
    "# count the number of snippets in each cell\n",
    "cell_to_count = defaultdict(int)\n",
    "for snippet in covered_snippets:\n",
    "    P = output_reader.P(snippet)\n",
    "    F = output_reader.F(snippet, _log=True)\n",
    "    b1 = int(np.digitize(P, bins_P))\n",
    "    b2 = int(np.digitize(F, bins_F))\n",
    "    cell_to_count[(b1, b2)] += 1\n",
    "    \n",
    "# compute the anomaly scores for each snippet\n",
    "P_scores = list()\n",
    "F_scores = list()\n",
    "colors = list()\n",
    "for snippet in covered_snippets:\n",
    "    P = output_reader.P(snippet)\n",
    "    F = output_reader.F(snippet, _log=True)\n",
    "    b1 = int(np.digitize(P, bins_P))\n",
    "    b2 = int(np.digitize(F, bins_F))\n",
    "    P_scores.append(P)\n",
    "    F_scores.append(F)\n",
    "    # in our case here, we'll mark all the points that landed in a cell on their own as anomalies\n",
    "    if cell_to_count[(b1, b2)] == 1:\n",
    "        colors.append('coral')\n",
    "    else:\n",
    "        colors.append((0.5019607843137255, 0.5019607843137255, 0.5019607843137255, 0.2)) # 0.2 alpha gray    \n",
    "        \n",
    "# plot the anomalies\n",
    "plotter.x_vs_y(x=P_scores, \n",
    "               y=F_scores,\n",
    "               title='Stackoverflow PvF Plot',\n",
    "               yscale='symlog',\n",
    "               xlabel='Persistence',\n",
    "               ylabel='Frequency',\n",
    "               dpi=350,\n",
    "               size=350,\n",
    "               colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll pick three snippets to zoom in on; two outliers and one from the center of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Three snippets hand-picked by visualy inspecting the PvF plot.\n",
    "'''\n",
    "output_reader.load_occs({'1_23354_23354_commented_on_a',\n",
    "                         '1_72603_82199_commented_on_a|1_72603_82199_commented_on_a|1_72603_82199_commented_on_a',\n",
    "                         '1_1950_55747_answered'})\n",
    "t = output_reader.te - output_reader.ts + 1\n",
    "random.seed(0)\n",
    "P_scores = list()\n",
    "F_scores = list()\n",
    "colors = list()\n",
    "for pattern in covered_snippets.union({'1_23354_23354_commented_on_a',\n",
    "                         '1_72603_82199_commented_on_a|1_72603_82199_commented_on_a|1_72603_82199_commented_on_a',\n",
    "                         '1_1950_55747_answered'}):\n",
    "    P = output_reader.P(pattern)\n",
    "    F = output_reader.F(pattern)\n",
    "    P_scores.append(P)\n",
    "    F_scores.append(F)\n",
    "    if pattern == '1_23354_23354_commented_on_a':\n",
    "        colors.append(black)\n",
    "        occs = output_reader.occs(pattern)\n",
    "        plotter.timeline(occs,\n",
    "                         ts=1,\n",
    "                         te=t,\n",
    "                         t=t,\n",
    "                         xlabel='',\n",
    "                         interval='M',\n",
    "                         alpha=0.5,\n",
    "                         timeticksize=1,\n",
    "                         color=black,\n",
    "                         marker='line')\n",
    "    elif pattern == '1_72603_82199_commented_on_a|1_72603_82199_commented_on_a|1_72603_82199_commented_on_a':\n",
    "        colors.append(orange)\n",
    "        occs = output_reader.occs(pattern)\n",
    "        plotter.timeline(occs,\n",
    "                         ts=1 + (86400 * 9 + 50000), # zoom in\n",
    "                         te=t - (86400 * 95 + 10000), # zoom in\n",
    "                         t=t,\n",
    "                         xlabel='',\n",
    "                         interval='M',\n",
    "                         alpha=0.5,\n",
    "                         timeticksize=1,\n",
    "                         color=orange,\n",
    "                         marker='line')\n",
    "    elif pattern == '1_1950_55747_answered':\n",
    "        colors.append(blue)\n",
    "        occs = output_reader.occs(pattern)\n",
    "        plotter.timeline(occs,\n",
    "                         ts=1,\n",
    "                         te=t,\n",
    "                         t=t,\n",
    "                         xlabel='',\n",
    "                         interval='M',\n",
    "                         alpha=1.0,\n",
    "                         timeticksize=3,\n",
    "                         color=blue,\n",
    "                         marker='line')\n",
    "    else:\n",
    "        colors.append(gray)\n",
    "        \n",
    "plotter.x_vs_y(x=P_scores, \n",
    "               y=F_scores,\n",
    "               title='Stackoverflow PvF Plot',\n",
    "               yscale='symlog',\n",
    "               xlabel='Persistence',\n",
    "               ylabel='Frequency',\n",
    "               dpi=350,\n",
    "               size=350,\n",
    "               colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bursty anomaly (orange) reveals users u72603 and u82199 (anonymized ids) commenting on u82199’s answer 36 times over the course of 1hr in the 3-month interval, then never interacting again. The timeline we plotted is zoomed in on for clarity. The most persistent snippets (one shown in black) are users commenting on their own answers, suggesting that it is unusual for the same two distinct users to interact persistently over time. On the other hand, the subtly persistent anomaly (blue) reveals u1950 regularly answering u55747’s questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will analyze some scalability considerations. There are multiple such considerations, including how many edges we process, how large of patterns we discover (max size or `k_max` in the paper), and how long we allow the patterns to take to form (the window size or `delta_max` in the paper).\n",
    "\n",
    "We'll run PENminer, this time in a streaming fashion (i.e., sPENminer) to generate plots for three different values of `k_max`: `1, 2, 3`. In each plot, we will vary `delta_max` over `60, 300, 600, 900, 1800` in seconds. In principle, we should perform these runs multiple times and average, but in this tutorial we'll just run once so it doesn't take too long.\n",
    "\n",
    "The x-axis of each plot varies the maximum duration, and the y-axis shows the number of edges processed per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_name = 'stackoverflow'\n",
    "params_to_time = dict()\n",
    "max_durations = [60, 300, 600, 900, 1800]\n",
    "for max_size in [1, 2, 3]:\n",
    "    edges_per_secs = list()\n",
    "    for max_duration in max_durations:\n",
    "        stream = Stream(stream_name)\n",
    "        method = sPENminer(stream,\n",
    "                           window_size=max_duration,\n",
    "                           max_size=max_size,\n",
    "                           view='id',\n",
    "                           alpha=2.0,\n",
    "                           beta=0.5,\n",
    "                           gamma=3.0,\n",
    "                           save_output=False)\n",
    "        ts = time()\n",
    "        method.mine(verbose=False)\n",
    "        te = time()\n",
    "        # edges processed per second—the stream has 500000 edges in it\n",
    "        edges_per_sec = 500000 / (te - ts)\n",
    "        edges_per_secs.append(edges_per_sec)\n",
    "        params_to_time[(max_size, max_duration)] = edges_per_sec\n",
    "    plotter.x_vs_y(x=max_durations, \n",
    "                   y=edges_per_secs,\n",
    "                   title='Processing times with max size = {}'.format(max_size),\n",
    "                   xlabel='Edges Per Sec',\n",
    "                   ylabel='Max Duration (Sec)',\n",
    "                   linewidth=10,\n",
    "                   size=30,\n",
    "                   dpi=350,\n",
    "                   alpha=0.9,\n",
    "                   colors=[orange],\n",
    "                   xticks=[60, 300, 600, 900, 1800],\n",
    "                   yticks=[1000, 10000, 25000, 50000, 75000, 100000, 150000],\n",
    "                   ytick_labels=['1K', '10K', '25K', '50K', '75K', '100K'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stackoverflow, edges only appear at a rate of `0.27` per second. Thus, with sPENminer processing `thousands` to `hundreds of thousands` of edges per second, it has no problem keeping up with this stream over these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
